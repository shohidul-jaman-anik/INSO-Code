import Anthropic from '@anthropic-ai/sdk';
import { InMemoryChatMessageHistory } from '@langchain/core/chat_history';
import { AIMessage, HumanMessage } from '@langchain/core/messages';
import httpStatus from 'http-status';
import { BufferMemory } from 'langchain/memory';
import ApiError from '../../../errors/ApiError.js';
import { logger } from '../../../shared/logger.js';
import UserModel from '../auth/auth.model.js';
import Llama from '../groq/groq.model.js';
import { paymentController } from '../payment/payment.controller.js';

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

// ✅ In-memory session memory store
const sessionMemoryStore = {};

export const claudeResponseService = async (prompt, userId, sessionId) => {
  let memory = sessionMemoryStore[sessionId];
  if (!memory) {
    memory = new BufferMemory({
      returnMessages: true,
      memoryKey: 'history',
      chatHistory: new InMemoryChatMessageHistory(),
    });
    sessionMemoryStore[sessionId] = memory;
  }

  try {
    // Add the new human message to memory
    await memory.chatHistory.addMessage(new HumanMessage(prompt));

    // Gather message history for Claude
    const previousMessages = await memory.chatHistory.getMessages();

    // Claude expects messages as { role: "user" | "assistant", content: "..." }
    const messages = previousMessages.map(msg => ({
      role: msg._getType() === 'human' ? 'user' : 'assistant',
      content: msg.content,
    }));

    const startTime = Date.now();

    // ✅ Claude API call
    const completion = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20241022', // latest Claude 3.5 model
      max_tokens: 1000,
      messages: [...messages, { role: 'user', content: prompt }],
    });

    const endTime = Date.now();
    const totalTime = endTime - startTime;

    const reply =
      completion.content?.[0]?.text || 'No response generated by Claude.';

    // ✅ Payment tracking (kept as-is)
    try {
      const paymentResult =
        await paymentController.incrementPromptsUsed(userId);
      if (!paymentResult.success) {
        throw new ApiError(httpStatus.BAD_REQUEST, paymentResult.message);
      }
    } catch (error) {
      logger.error('Error in incrementPromptsUsed:', error);
      throw new ApiError(
        httpStatus.INTERNAL_SERVER_ERROR,
        error.message || 'An error occurred while updating prompt usage.',
      );
    }

    // Add AI message to session memory
    await memory.chatHistory.addMessage(new AIMessage(reply));

    // ✅ Same database structure as before
    const responseData = {
      prompt,
      model: 'claude-3-5-sonnet-20241022',
      reply,
      total_time: totalTime,
    };

    let claudeSession = await Llama.findOne({ user: userId, sessionId });

    if (claudeSession) {
      claudeSession.responses.push(responseData);
      await claudeSession.save();
    } else {
      claudeSession = await Llama.create({
        user: userId,
        sessionId,
        responses: [responseData],
      });
      await UserModel.findByIdAndUpdate(userId, {
        $push: { llamaAiSessions: claudeSession._id },
      });
    }

    // ✅ Redis publish (kept same)
    const payload = { prompt, sessionId, reply };
    // if (payload) {
    //   await RedisClient.publish(
    //     DEEPSEEK_RESPONSE_SERVICE_POST,
    //     JSON.stringify(payload),
    //   );
    // }

    return payload;
  } catch (error) {
    logger.error('Error in claudeResponseService:', error);
    throw new ApiError(
      httpStatus.INTERNAL_SERVER_ERROR,
      'Claude service failed.',
    );
  }
};

export const deepseekServices = {
  deepseekResponseService,
};
